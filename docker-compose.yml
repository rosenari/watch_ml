services:
  backend:
    container_name: backend
    build:
      context: ./backend
      dockerfile: Dockerfile.dev
    ports:
      - '5000:5000'
    volumes:
      - './backend/app:/src/app'
      - './backend/dataset_archive:/src/dataset_archive'
      - './backend/main.py:/src/main.py'
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
      - PYTHONUNBUFFERED=1
      - DATASET_DIRECTORY=/src/dataset_archive
      - MODEL_DIRECTORY=/src/runs/model_repo
      - DATABASE_USER=mluser
      - DATABASE_PASSWORD=devpassword
      - DATABASE_HOST=db
      - DATABASE=watchml
    command: uvicorn main:app --host 0.0.0.0 --port 5000 --reload
    networks:
      - monitoring_network
    depends_on:
      redis:
        condition: service_healthy
      db:
        condition: service_healthy
  celery_worker:
    container_name: worker
    build:
      context: ./backend
      dockerfile: Dockerfile.worker
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    depends_on:
      redis:
        condition: service_healthy
    volumes:
      - './backend/app:/src/app'
      - './backend/dataset_archive:/src/dataset_archive'
      - './backend/mlruns:/src/runs'
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_ARCHIVE_PATH=/src/dataset_archive
      - VALID_ARCHIVE_MODULE_PATH=valid_archive
      - CELERY_ML_RUNS_PATH=/src/runs
      - DATASET_DIRECTORY=/src/dataset_archive
      - MODEL_DIRECTORY=/src/runs/model_repo
      - DATABASE_USER=mluser
      - DATABASE_PASSWORD=devpassword
      - DATABASE_HOST=db
      - DATABASE=watchml
    runtime: nvidia
    shm_size: '1g'
    command: celery -A app.tasks.main worker --loglevel=info --concurrency=1 --pool=threads
    networks:
      - monitoring_network
  redis:
    image: redis:7.4
    container_name: redis
    ports:
      - '6379:6379'
    restart: always
    healthcheck:
      test: ['CMD', 'redis-cli', 'ping']
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - monitoring_network
  db:
    image: postgres:14
    environment:
      POSTGRES_DB: 'watchml'
      POSTGRES_USER: 'mluser'
      POSTGRES_PASSWORD: 'devpassword'
      POSTGRES_ROOT_PASSWORD: 'devpassword'
      TZ: Asia/Seoul
    volumes:
      - ./backend/db/initdb.d:/docker-entrypoint-initdb.d
      - ./backend/db/data:/var/lib/postgresql/data
    ports:
      - '5432:5432'
    networks:
      - monitoring_network
    restart: always
    healthcheck:
      test: ['CMD-SHELL', 'pg_isready -U postgres']
      interval: 5s
      timeout: 5s
      retries: 5
  triton:
    image: nvcr.io/nvidia/tritonserver:24.08-py3
    container_name: triton_server
    ports:
      - '8000:8000'
      - '8001:8001'
      - '8002:8002'
    volumes:
      - './backend/mlruns:/src/runs'
    environment:
      - MODEL_REPOSITORY=/src/runs/triton_repo # 모델 저장소 경로 설정
    runtime: nvidia # GPU 지원을 위해 nvidia runtime 사용
    shm_size: '2g'
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu] # GPU 사용 설정
    command: /bin/bash -c "mkdir -p /src/runs/triton_repo && tritonserver --model-repository=/src/runs/triton_repo"
    networks:
      - monitoring_network

networks:
  monitoring_network:
