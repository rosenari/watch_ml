services:
  backend:
    container_name: backend
    build:
      context: ./backend
      dockerfile: Dockerfile.dev
    ports:
      - '5000:5000'
    volumes:
      - './backend/app:/src/app'
      - './backend/dataset_archive:/src/dataset_archive'
      - './backend/main.py:/src/main.py'
      - './backend/mlruns:/src/runs'
      - './backend/inference_files:/src/inference_files'
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
      - PYTHONUNBUFFERED=1
      - DATASET_DIRECTORY=/src/dataset_archive
      - MODEL_DIRECTORY=/src/runs/model_repo
      - INFERENCE_DIRECTORY=/src/inference_files
      - DATABASE_USER=mluser
      - DATABASE_PASSWORD=devpassword
      - DATABASE_HOST=db
      - DATABASE=watchml
    command: uvicorn main:app --host 0.0.0.0 --port 5000 --reload
    networks:
      - monitoring_network
    depends_on:
      redis:
        condition: service_healthy
      db:
        condition: service_healthy
      fluentd:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
    logging:
      driver: fluentd
      options:
        fluentd-address: localhost:24224
        tag: backend
  celery_worker:
    container_name: worker
    build:
      context: ./backend
      dockerfile: Dockerfile.worker
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    depends_on:
      redis:
        condition: service_healthy
      db:
        condition: service_healthy
      fluentd:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
    logging:
      driver: fluentd
      options:
        fluentd-address: localhost:24224
        tag: celery_worker
    volumes:
      - './backend/app:/src/app'
      - './backend/dataset_archive:/src/dataset_archive'
      - './backend/mlruns:/src/runs'
      - './backend/inference_files:/src/inference_files'
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_ARCHIVE_PATH=/src/dataset_archive
      - VALID_ARCHIVE_MODULE_PATH=valid_archive
      - CELERY_ML_RUNS_PATH=/src/runs
      - DATASET_DIRECTORY=/src/dataset_archive
      - INFERENCE_DIRECTORY=/src/inference_files
      - MODEL_DIRECTORY=/src/runs/model_repo
      - DATABASE_USER=mluser
      - DATABASE_PASSWORD=devpassword
      - DATABASE_HOST=db
      - DATABASE=watchml
      - TRITON_GRPC_URL=triton:8001
    runtime: nvidia
    shm_size: '1g'
    command: celery -A app.tasks.main worker --loglevel=info --concurrency=1 --pool=threads
    networks:
      - monitoring_network
  redis:
    image: redis:7.4
    container_name: redis
    ports:
      - '6379:6379'
    restart: always
    healthcheck:
      test: ['CMD', 'redis-cli', 'ping']
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - monitoring_network
  db:
    image: postgres:14
    container_name: db
    environment:
      POSTGRES_DB: 'watchml'
      POSTGRES_USER: 'mluser'
      POSTGRES_PASSWORD: 'devpassword'
      POSTGRES_ROOT_PASSWORD: 'devpassword'
      TZ: Asia/Seoul
    volumes:
      - ./backend/db/initdb.d:/docker-entrypoint-initdb.d
      - ./backend/db/data:/var/lib/postgresql/data
    ports:
      - '5432:5432'
    networks:
      - monitoring_network
    restart: always
    healthcheck:
      test: ['CMD-SHELL', 'pg_isready -U mluser -d watchml']
      interval: 5s
      timeout: 5s
      retries: 5
  triton:
    image: nvcr.io/nvidia/tritonserver:24.08-py3
    container_name: triton_server
    restart: always
    ports:
      - '8000:8000'
      - '8001:8001'
      - '8002:8002'
    volumes:
      - './backend/mlruns:/src/runs'
    environment:
      - MODEL_REPOSITORY=/src/runs/triton_repo # 모델 저장소 경로 설정
    runtime: nvidia # GPU 지원을 위해 nvidia runtime 사용
    shm_size: '2g'
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu] # GPU 사용 설정
    command: /bin/bash -c "mkdir -p /src/runs/triton_repo && tritonserver --model-repository=/src/runs/triton_repo --model-control-mode=explicit"
    networks:
      - monitoring_network
  fluentd:
    build:
      context: ./fluentd
    container_name: fluentd
    ports:
      - "24224:24224"
      - "24224:24224/udp"
    volumes:
      - ./fluentd/fluentd.conf:/fluentd/etc/fluent.conf
      - ./fluentd/fluentd.conf:/etc/fluent/fluent.conf
    networks:
      - monitoring_network
    healthcheck:
      test: ["CMD", "fluentd", "--dry-run"]
      interval: 5s
      retries: 5
  kibana:
    image: docker.elastic.co/kibana/kibana:7.15.0
    container_name: kibana
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_URL=http://elasticsearch:9200
      - ELASTICSEARCH_USERNAME=elastic
      - ELASTICSEARCH_PASSWORD=devpassword
    depends_on:
      elasticsearch:
        condition: service_healthy
    networks:
      - monitoring_network
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.15.0
    container_name: search
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=true
      - ELASTIC_PASSWORD=devpassword
    ports:
      - "9200:9200"
    networks:
      - monitoring_network
    healthcheck:
      test: ["CMD-SHELL", "curl -u 'elastic:devpassword' -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 10s
      retries: 10

networks:
  monitoring_network:
